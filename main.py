import json
import os
import re
import sys
from datetime import datetime, timedelta
import gspread
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build

SCOPES = ['https://www.googleapis.com/auth/spreadsheets']

def read_config(file_path):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã©ã‚’èª­ã¿è¾¼ã¿ã€APIã‚­ãƒ¼ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹"""
    # APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•° 'YOUTUBE_API_KEY' ã‹ã‚‰å–å¾—
    api_key = os.environ.get("YOUTUBE_API_KEY")
    if not api_key:
        # ç’°å¢ƒå¤‰æ•°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã¦ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'YOUTUBE_API_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)

    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨é–‹å§‹æ—¥æ™‚ã‚’èª­ã¿è¾¼ã‚€
    with open(file_path, 'r', encoding='utf-8') as file:
        config = json.load(file)
    
    # configãƒ•ã‚¡ã‚¤ãƒ«å†…ã«api_keyãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ç„¡è¦–ã•ã‚Œã‚‹ï¼ˆç’°å¢ƒå¤‰æ•°ãŒå„ªå…ˆï¼‰
    return api_key, config['keywords'], config['start_datetime']

def jst_to_utc(jst_str):
    """JSTæ—¥æ™‚æ–‡å­—åˆ—ã‚’UTCã®ISO8601ã«å¤‰æ›"""
    jst_dt = datetime.strptime(jst_str, "%Y-%m-%d %H:%M:%S")
    utc_dt = jst_dt - timedelta(hours=9)
    return utc_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

def iso8601_to_duration(iso_duration):
    """PTè¡¨è¨˜ï¼ˆYouTube ISO8601ï¼‰ã‚’HH:MM:SSåŒ–"""
    pattern = re.compile(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?')
    match = pattern.match(iso_duration)
    if not match:
        return "00:00:00"
    hours = int(match.group(1)) if match.group(1) else 0
    minutes = int(match.group(2)) if match.group(2) else 0
    seconds = int(match.group(3)) if match.group(3) else 0
    return str(timedelta(hours=hours, minutes=minutes, seconds=seconds))

def convert_to_japan_time(utc_time):
    """UTCæ™‚åˆ»ã‚’JSTå¤‰æ›ã—è¡¨ç¤ºç”¨ã«"""
    utc_datetime = datetime.strptime(utc_time, "%Y-%m-%dT%H:%M:%SZ")
    japan_datetime = utc_datetime + timedelta(hours=9)
    return japan_datetime.strftime("%Y/%m/%d %H:%M:%S")

def get_current_japan_time():
    """ç¾åœ¨æ™‚åˆ» (JSTè¡¨ç¤º)"""
    now_utc = datetime.utcnow()
    now_jst = now_utc + timedelta(hours=9)
    return now_jst.strftime("%Y/%m/%d %H:%M:%S")

def get_current_japan_digit_date():
    """ä»Šæ—¥ã®æ—¥ä»˜ (JST, ã‚·ãƒ¼ãƒˆåç”¨ 'YYYYMMDD' ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)"""
    now_utc = datetime.utcnow()
    now_jst = now_utc + timedelta(hours=9)
    return now_jst.strftime("%Y%m%d")

def calc_engagement_rate(like_count, comment_count, view_count):
    """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ (ï¼…)"""
    if view_count == 0:
        return 0.0
    return round((like_count + comment_count) / view_count * 100, 2)

def get_youtube_data(api_key, keyword, start_datetime_jst, end_datetime_jst, max_total_results=100):
    """
    æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»æœŸé–“ã®YouTubeå‹•ç”»æƒ…å ±ã‚’100ä»¶ä¸Šé™ã§å–å¾—
    """
    youtube = build('youtube', 'v3', developerKey=api_key)
    start_utc = jst_to_utc(start_datetime_jst)
    end_utc = jst_to_utc(end_datetime_jst)
    start_dt = datetime.strptime(start_datetime_jst, "%Y-%m-%d %H:%M:%S")
    end_dt = datetime.strptime(end_datetime_jst, "%Y-%m-%d %H:%M:%S")

    video_ids = []
    next_page_token = None

    while len(video_ids) < max_total_results:
        search_response = youtube.search().list(
            q=keyword,
            part='snippet',
            type='video',
            maxResults=min(50, max_total_results - len(video_ids)),
            publishedAfter=start_utc,
            publishedBefore=end_utc,
            pageToken=next_page_token
        ).execute()

        video_ids += [item['id']['videoId'] for item in search_response['items']]
        next_page_token = search_response.get('nextPageToken')
        if not next_page_token or len(video_ids) >= max_total_results:
            break

    video_data = []
    for i in range(0, len(video_ids), 50):
        batch_ids = video_ids[i:i+50]
        video_response = youtube.videos().list(
            part='snippet,statistics,contentDetails',
            id=','.join(batch_ids)
        ).execute()

        for item in video_response['items']:
            snippet = item['snippet']
            statistics = item.get('statistics', {})
            content_details = item['contentDetails']

            published_at_utc = snippet['publishedAt']
            published_at_jst = datetime.strptime(published_at_utc, "%Y-%m-%dT%H:%M:%SZ") + timedelta(hours=9)

            # å³å¯†ãªæ™‚é–“ãƒã‚§ãƒƒã‚¯ï¼ˆYouTube APIã®publishedBefore/Afterã¯å¤šå°‘æ›–æ˜§ãªãŸã‚ï¼‰
            if not (start_dt <= published_at_jst <= end_dt):
                continue

            # 'likeCount'ã‚„'commentCount'ãŒå­˜åœ¨ã—ãªã„å ´åˆãŒã‚ã‚‹ãŸã‚getã‚’ä½¿ç”¨
            video_data.append({
                'title': snippet['title'],
                'channel': snippet['channelTitle'],
                'published_at': snippet['publishedAt'],
                'video_id': item['id'],
                'view_count': int(statistics.get('viewCount', 0)),
                'like_count': int(statistics.get('likeCount', 0)),
                'comment_count': int(statistics.get('commentCount', 0)),
                'duration': content_details.get('duration', "PT0S")
            })

    return video_data

def merge_and_deduplicate(video_data_list, keywords):
    """é‡è¤‡å‰Šé™¤ï¼‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã«å«ã‚€å‹•ç”»ã®ã¿æŠ½å‡º"""
    merged = {}
    for video_data in video_data_list:
        for video in video_data:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if any(k in video['title'] for k in keywords):
                merged[video['video_id']] = video
    return list(merged.values())

def export_to_google_sheet(video_data, spreadsheet_id, exec_time_jst, sheet_name):
    """
    Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›ï¼ˆæ–°è¦ã‚·ãƒ¼ãƒˆä½œæˆã—ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼‰
    """
    # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼ (GCP_SERVICE_ACCOUNT_KEYã¯ç’°å¢ƒå¤‰æ•°/Secretsã‹ã‚‰å–å¾—)
    credentials_dict = json.loads(os.environ["GCP_SERVICE_ACCOUNT_KEY"])
    creds = Credentials.from_service_account_info(credentials_dict, scopes=SCOPES)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(spreadsheet_id)

    # æ–°ã—ã„ã‚·ãƒ¼ãƒˆã‚’ä½œæˆ
    worksheet = sh.add_worksheet(title=sheet_name, rows="100", cols="20")

    headers = [
        "å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«", "ãƒãƒ£ãƒ³ãƒãƒ«å", "æŠ•ç¨¿æ—¥æ™‚ï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰", "å‹•ç”»ID",
        "å‹•ç”»URL", "å†ç”Ÿå›æ•°", "é«˜è©•ä¾¡æ•°", "è¦–è´è€…ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "å‹•ç”»ã®é•·ã•",
        "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡(%)", "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œæ™‚é–“ï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰"
    ]
    rows = []
    for video in video_data:
        engagement_rate = calc_engagement_rate(video['like_count'], video['comment_count'], video['view_count'])
        video_url = f"https://www.youtube.com/watch?v={video['video_id']}"
        rows.append([
            video['title'],
            video['channel'],
            convert_to_japan_time(video['published_at']),
            video['video_id'],
            video_url,
            video['view_count'],
            video['like_count'],
            video['comment_count'],
            iso8601_to_duration(video['duration']),
            engagement_rate,
            exec_time_jst
        ])
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ¼ãƒˆã«è¿½åŠ 
    worksheet.append_row(headers)
    worksheet.append_rows(rows, value_input_option='USER_ENTERED')

def main():
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å
    config_file = 'å‹•ç”»ãƒªã‚¹ãƒˆconfig.txt'
    # è‡ªèº«ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID
    spreadsheet_id = '1MloHGh089FVzMxP5migrOpHz5VkGuQ-W0-8Ki9MUhdU'

    # è¨­å®šã¨APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆAPIã‚­ãƒ¼ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
    api_key, keywords, start_datetime_jst = read_config(config_file)

    # ä»Šæ—¥ã®æ—¥ä»˜ã¨å®Ÿè¡Œæ™‚é–“ã‚’JSTã§å–å¾—
    sheet_name = get_current_japan_digit_date()
    exec_time_jst = get_current_japan_time()
    
    # æ¤œç´¢çµ‚äº†æ—¥æ™‚ã‚’ä»Šæ—¥ã®10:01:00 JSTã«è¨­å®š
    end_datetime_jst = f"{sheet_name[:4]}-{sheet_name[4:6]}-{sheet_name[6:]} 10:01:00"

    # --- ã‚·ãƒ¼ãƒˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆAPIã‚¢ã‚¯ã‚»ã‚¹å‰ï¼‰ ---
    # GCPã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼
    try:
        credentials_dict = json.loads(os.environ["GCP_SERVICE_ACCOUNT_KEY"])
    except KeyError:
        print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° 'GCP_SERVICE_ACCOUNT_KEY' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
        
    creds = Credentials.from_service_account_info(credentials_dict, scopes=SCOPES)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(spreadsheet_id)
    existing_sheets = [ws.title for ws in sh.worksheets()]
    
    if sheet_name in existing_sheets:
        print(f"âœ… {sheet_name}ã‚·ãƒ¼ãƒˆã¯æ—¢ã«å­˜åœ¨ã—ã¦ã„ã‚‹ãŸã‚APIã‚¢ã‚¯ã‚»ã‚¹ã›ãšã«ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    # --- ä»¥é™ã®ã¿YouTube Data APIã‚¢ã‚¯ã‚»ã‚¹ ---
    video_data_list = []
    print(f"â¡ï¸ YouTubeãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {len(keywords)}ä»¶, æœŸé–“: {start_datetime_jst} ã€œ {end_datetime_jst})")
    for keyword in keywords:
        video_data = get_youtube_data(api_key, keyword, start_datetime_jst, end_datetime_jst, max_total_results=100)
        video_data_list.append(video_data)
        print(f"   - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keyword}': {len(video_data)}ä»¶å–å¾—")

    # ãƒ‡ãƒ¼ã‚¿çµ±åˆã€é‡è¤‡æ’é™¤ã€ã‚¿ã‚¤ãƒˆãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    merged_video_data = merge_and_deduplicate(video_data_list, keywords)
    print(f"â¡ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»é‡è¤‡æ’é™¤å¾Œ: {len(merged_video_data)}ä»¶")
    
    # å†ç”Ÿå›æ•°ã§ã‚½ãƒ¼ãƒˆ
    merged_video_data.sort(key=lambda x: x['view_count'], reverse=True)
    
    # Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›
    export_to_google_sheet(merged_video_data, spreadsheet_id, exec_time_jst, sheet_name)
    print(f"ğŸ‰ å‡¦ç†å®Œäº†ï¼ˆã‚·ãƒ¼ãƒˆå: {sheet_name}ã€å‹•ç”»æ•°: {len(merged_video_data)}ä»¶ï¼‰")

if __name__ == "__main__":
    main()
